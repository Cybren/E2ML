{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39c5f71d",
   "metadata": {},
   "source": [
    "# Foundations of Machine Learning\n",
    "\n",
    "In this notebook, we will learn about the basics of machine learning. \n",
    "\n",
    "At the start, we will implement various **loss and risk** estimation functions.\n",
    "\n",
    "Subsequently, we will implement a **binary logistic regression** (BLR) model according to the [scikit-learn guidelines](https://scikit-learn.org/stable/developers/develop.html).\n",
    "\n",
    "Further, we will introduce **nonlinear basis functions** as a prerequisite for nonlinear decision boundaries when using BLR models.\n",
    "\n",
    "Finally, we apply BLR models to an artificial data set to investigate the **under- and overfitting** issue.\n",
    "\n",
    "### **Table of Contents**\n",
    "1. [Loss Functions and Notions of Risk](#loss-functions-and-notions-of-risk)\n",
    "2. [Binary Logistic Regression](#binary-logistic-regression)\n",
    "3. [Nonlinear Basis Functions](#basis-functions)\n",
    "4. [Under- and Overfitting](#under-overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a714745d-6c84-452f-b796-681088b60161",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from math import isclose\n",
    "from ipywidgets import interactive, IntSlider, FloatSlider"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e2e7e46",
   "metadata": {},
   "source": [
    "### **1. Loss Functions and Notions of Risk** <a class=\"anchor\" id=\"loss-functions-and-notions-of-risk\"></a>\n",
    "\n",
    "The most popular loss function in classification is the zero-one loss function $L_{0/1}$, which is given by\n",
    "\n",
    "$L_{0/1}(y, y') = \\delta(y \\neq y')= 0$ if $y = y'$ $1$ else\n",
    "\n",
    "#### **Question:**\n",
    "1. (a) What are the reasons that the zero-one loss function $L_{0/1}$ is typically used for assessing a classifier but not for training?\n",
    "\n",
    "   Non-Convex and Non-smooth, because optimal solution ist not guaranteed and gradients are either 0 or $\\infty$\n",
    "   \n",
    "Alternative loss functions have been proposed that are suitable for optimizing a classifier during training. One of such functions is the *binary cross entropy* (BCE) loss function $L_\\mathrm{BCE}$, which is given by:\n",
    "\n",
    "$L_\\mathrm{BCE} (p, p') = -p\\ln(p') - (1 - p) \\ln(1 - p')$\n",
    "\n",
    "Allgemein: \n",
    "\n",
    "$L_\\mathrm{BCE} (P, Q) = -\\Sigma_{x \\in \\mathcal{X} } P(x) \\ln(Q(x))$ what is X?\n",
    "\n",
    "#### **Question:**\n",
    "1. (b) Is the BCE loss function convex with respect to the estimated probability? Prove your answer.\n",
    "\n",
    "   *Remark: A function $f$ is convex in an interval $\\mathcal{I}$, if $\\forall x \\in \\mathcal{I}: f^{\\prime\\prime}(x) \\geq 0$.*\n",
    "\n",
    "   Ableitung $ \\ln(x) = \\frac{1}{x}$\n",
    "   \n",
    "   $f'(x) = \\frac{1-p}{1 - p'} - \\frac{p}{p'}= -\\frac{p'-p}{(p' - 1)p'}$\n",
    "\n",
    "   Ableitung $\\frac{u(x)}{v(x)} = \\frac{u'(x)v(x) - u(x)v'(x)}{v(x)^2}$\n",
    "\n",
    "   Setze $p' = x$\n",
    "\n",
    "   $f''(p, x) = \\frac{(x - p)'((x - 1)x) - (x - p)((x - 1) x)'}{((x - 1)p)^2} = \\frac{((x - 1)x) - (x - p) ((x - 1)' x + (x - 1) x')}{((x - 1)p)^2} = \\frac{((x - 1)x) - (x - p) (x + (x - 1))}{((x - 1)p)^2} = \\frac{(x^2 - x) - (x - p) (2x - 1)}{((x - 1)p)^2} = \\frac{(x^2 - x) - (2x^2 - x -2xp + p)}{((x - 1)p)^2}$\n",
    "\n",
    "   $f''(p,x) = \\frac{1-p}{(1-x)^2} + \\frac{x}{x^2}$\n",
    "   Alway non negative, cause $p \\in [0,1]$ and $p' \\in (0,1)$ \n",
    "   \n",
    "For a given loss function $L$ and a dataset $\\mathcal{D}$, the empirical risk $R_{\\mathcal{D}}(h)$ of a classifier $h$ is given by:\n",
    "\n",
    "$R_{\\mathcal{D}}(h) = \\frac{1}{|\\mathcal{D}|}\\Sigma_{(\\mathbf(x),y) \\in \\mathcal{D}} L(y, h(\\mathbf{x}))$\n",
    "\n",
    "With this knowledge, we implement the empirical risk for the zero-one loss function $L_{0/1}$ and the BCE loss function $L_{\\mathrm{BCE}}$ in the Python file [`e2ml.evaluation._loss_functions`](../e2ml/evaluation/_loss_functions.py). Afterwards, we check our code by executing the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8377f885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3194030804871087\n"
     ]
    }
   ],
   "source": [
    "from e2ml.evaluation import zero_one_loss, binary_cross_entropy_loss\n",
    "\n",
    "y_true = [0, 1, 0, 1, 0]\n",
    "y_pred = [0, 0, 0, 1, 0]\n",
    "check = zero_one_loss(y_true, y_pred) == 0.2\n",
    "assert check, 'The empirical risk must be 0.2 for this data set.' \n",
    "\n",
    "y_true = [0, 1, 0, 1, 0]\n",
    "y_pred = [0.00000001, 0.9, 0.1, 0.5, 0.5]\n",
    "print(binary_cross_entropy_loss(y_true, y_pred))\n",
    "check = isclose(binary_cross_entropy_loss(y_true, y_pred), 0.319, abs_tol=0.001)\n",
    "assert check, 'The empirical risk must be around 0.319 for this data set.' \n",
    "\n",
    "y_true_list = [[2], [-1], [0.5], [0.5]]\n",
    "y_pred_list = [[0.5], [0.5], [2], [-1]]\n",
    "for y_true, y_pred in zip(y_true_list, y_pred_list):\n",
    "    check = False\n",
    "    try:\n",
    "        binary_cross_entropy_loss(y_true, y_pred)\n",
    "    except ValueError:\n",
    "        check = True\n",
    "    assert check, 'There must be a ValueError because of invalid values.' "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca7473c8",
   "metadata": {},
   "source": [
    "### **2. Binary Logistic Regression** <a class=\"anchor\" id=\"binary-logistic-regression\"></a>\n",
    "\n",
    "*Binary logistic regression* (BLR) is a space of classifiers optimizing the BCE loss function. Instead of a class label, a BLR model with weights $\\mathbf{w} \\in \\mathbb{R}^M$ and $\\boldsymbol{\\phi}: \\mathbb{R}^D \\rightarrow \\mathbb{R}^M$ as a vector of $M$ basis functions estimates the conditional probability according to:\n",
    "\n",
    "\n",
    "$h_\\mathbf{w}(\\phi(\\mathbf{x})) = p(y = 1 | \\phi(\\mathbf{x}), \\mathbf{w}) = \\sigma(\\phi(\\mathbf{x})^T\\mathbf{w}) = \\frac{1}{1 + exp(-\\phi(\\mathbf{x}^T\\mathbf{w}))}$\n",
    "\n",
    "For a *regularized risk minimization* (RRM) algorithm, we train a BLR model by optimizing\n",
    "\n",
    "$R_\\mathcal{D}(\\mathbf{w}_t) + J(h_\\mathbf{w})$\n",
    "\n",
    "For optimization, we could use a gradient descent approach:\n",
    "\n",
    "$$\\mathbf{w}_{t+1} = \\mathbf{w}_t - \\eta \\cdot \\nabla\\left(R_\\mathcal{D}(\\mathbf{w}_t) + J(h_\\mathbf{w})\\right),$$\n",
    "\n",
    "where $\\nabla$ denotes the gradient, $t$ the current optimization step, and $\\eta \\in \\mathbb{R}_{>0}$ the learning rate defining the step size in each optimization step. The initial weights $\\mathbf{w}_0$ can be chosen randomly and the subsequent optimization steps can be repeated until a stopping criterion is reached, e.g., maximum number of steps. The gradient $\\nabla\\left(R_\\mathcal{D}(\\mathbf{w}_t) + J(h_\\mathbf{w})\\right)$ with respect to the weights $\\mathbf{w}$ is then given by:\n",
    "\n",
    "$L = - y \\log (h_w(\\phi(x))) - (1-y) \\log (1 - h_w(\\phi(x)))$\n",
    "\n",
    "J is weight decay $J(h_\\mathbf{w}) = \\frac{\\lambda}{2|D|} w^Tw$\n",
    "\n",
    "$\\nabla\\left(R_\\mathcal{D}(\\mathbf{w}_t) + J(h_\\mathbf{w})\\right) = \\left(\\nabla R_\\mathcal{D}(\\mathbf{w}_t) + \\nabla J(h_\\mathbf{w})\\right) = ... = (\\frac{1}{2|D|} \\Sigma ((h_W\\phi(x)) -y) * \\phi(x)) + \\frac{\\lambda}{|D|}w$\n",
    "\n",
    "With this knowledge, we implement the class [`BinaryLogisticRegression`](../e2ml/models/_binary_logistic_regression.py) in the [`e2ml.models`](../e2ml/models) subpackage. However, there we use [L-BFGS-B](https://en.wikipedia.org/wiki/Limited-memory_BFGS) as a so-called quasi-Netwon algorithm, which is an advancement of the standard gradient descent algorithm. Once, the implementation has been completed, we check its validity on a simple two-dimensional artificial dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a27f7ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 100 is different from 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39m# Fit `BinaryLogisticRegression` with `maxiter=1` and `lmbda=0` on the artificial data set.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m blr \u001b[39m=\u001b[39m BinaryLogisticRegression(maxiter\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, lmbda\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m blr\u001b[39m.\u001b[39;49mfit(X, y)\n\u001b[1;32m     11\u001b[0m \u001b[39m# Make predictions using the fitted `BinaryLogisticRegression` model on the training set `X`.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m y_pred \u001b[39m=\u001b[39m blr\u001b[39m.\u001b[39mpredict(X)\n",
      "File \u001b[0;32m~/Studium/git/E2ML/e2ml/models/_binary_logistic_regression.py:107\u001b[0m, in \u001b[0;36mBinaryLogisticRegression.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[39mreturn\u001b[39;00m gradient\n\u001b[1;32m    105\u001b[0m \u001b[39m# Use `scipy.optimize.minimize` with `BFGS` as `method` to optimize the loss function and store the result as\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[39m# `self.w_`\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mw_ \u001b[39m=\u001b[39m minimize(fun\u001b[39m=\u001b[39;49mloss_func, x0\u001b[39m=\u001b[39;49mw0, jac\u001b[39m=\u001b[39;49mgradient_func, method\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mBFGS\u001b[39;49m\u001b[39m'\u001b[39;49m, options\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mmaxiter\u001b[39;49m\u001b[39m\"\u001b[39;49m : \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmaxiter})\u001b[39m.\u001b[39mx\n\u001b[1;32m    109\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/e2ml-env/lib/python3.9/site-packages/scipy/optimize/_minimize.py:614\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[39mreturn\u001b[39;00m _minimize_cg(fun, x0, args, jac, callback, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[1;32m    613\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbfgs\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 614\u001b[0m     \u001b[39mreturn\u001b[39;00m _minimize_bfgs(fun, x0, args, jac, callback, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions)\n\u001b[1;32m    615\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnewton-cg\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    616\u001b[0m     \u001b[39mreturn\u001b[39;00m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    617\u001b[0m                               \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n",
      "File \u001b[0;32m~/anaconda3/envs/e2ml-env/lib/python3.9/site-packages/scipy/optimize/optimize.py:1135\u001b[0m, in \u001b[0;36m_minimize_bfgs\u001b[0;34m(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[39mif\u001b[39;00m maxiter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1133\u001b[0m     maxiter \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(x0) \u001b[39m*\u001b[39m \u001b[39m200\u001b[39m\n\u001b[0;32m-> 1135\u001b[0m sf \u001b[39m=\u001b[39m _prepare_scalar_function(fun, x0, jac, args\u001b[39m=\u001b[39;49margs, epsilon\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m   1136\u001b[0m                               finite_diff_rel_step\u001b[39m=\u001b[39;49mfinite_diff_rel_step)\n\u001b[1;32m   1138\u001b[0m f \u001b[39m=\u001b[39m sf\u001b[39m.\u001b[39mfun\n\u001b[1;32m   1139\u001b[0m myfprime \u001b[39m=\u001b[39m sf\u001b[39m.\u001b[39mgrad\n",
      "File \u001b[0;32m~/anaconda3/envs/e2ml-env/lib/python3.9/site-packages/scipy/optimize/optimize.py:261\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[0;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[1;32m    257\u001b[0m     bounds \u001b[39m=\u001b[39m (\u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39minf, np\u001b[39m.\u001b[39minf)\n\u001b[1;32m    259\u001b[0m \u001b[39m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[39m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m sf \u001b[39m=\u001b[39m ScalarFunction(fun, x0, args, grad, hess,\n\u001b[1;32m    262\u001b[0m                     finite_diff_rel_step, bounds, epsilon\u001b[39m=\u001b[39;49mepsilon)\n\u001b[1;32m    264\u001b[0m \u001b[39mreturn\u001b[39;00m sf\n",
      "File \u001b[0;32m~/anaconda3/envs/e2ml-env/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py:136\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[0;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf \u001b[39m=\u001b[39m fun_wrapped(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx)\n\u001b[1;32m    135\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_fun_impl \u001b[39m=\u001b[39m update_fun\n\u001b[0;32m--> 136\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_fun()\n\u001b[1;32m    138\u001b[0m \u001b[39m# Gradient evaluation\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcallable\u001b[39m(grad):\n",
      "File \u001b[0;32m~/anaconda3/envs/e2ml-env/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py:226\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_fun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    225\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated:\n\u001b[0;32m--> 226\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_fun_impl()\n\u001b[1;32m    227\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/e2ml-env/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py:133\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate_fun\u001b[39m():\n\u001b[0;32m--> 133\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf \u001b[39m=\u001b[39m fun_wrapped(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx)\n",
      "File \u001b[0;32m~/anaconda3/envs/e2ml-env/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py:130\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfun_wrapped\u001b[39m(x):\n\u001b[1;32m    129\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnfev \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 130\u001b[0m     \u001b[39mreturn\u001b[39;00m fun(x, \u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m~/Studium/git/E2ML/e2ml/models/_binary_logistic_regression.py:88\u001b[0m, in \u001b[0;36mBinaryLogisticRegression.fit.<locals>.loss_func\u001b[0;34m(w)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[39mCompute the (scaled) loss with respect to weights `w`.\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[39m    Evaluated (scaled) loss.\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[39m# Compute predictions for given weights.\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m y_pred \u001b[39m=\u001b[39m expit(np\u001b[39m.\u001b[39;49mmatmul(X, w))\n\u001b[1;32m     90\u001b[0m \u001b[39m# Compute binary cross entropy loss including regularization.\u001b[39;00m\n\u001b[1;32m     91\u001b[0m loss \u001b[39m=\u001b[39m binary_cross_entropy_loss(y_true\u001b[39m=\u001b[39my, y_pred\u001b[39m=\u001b[39my_pred)\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 100 is different from 2)"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "from e2ml.models import BinaryLogisticRegression\n",
    "\n",
    "# Create linearly separable dataset.\n",
    "X, y = make_blobs(n_samples=100, centers=[[-1, -1], [1, 1]], cluster_std=0.1, random_state=0)\n",
    "\n",
    "# Fit `BinaryLogisticRegression` with `maxiter=1` and `lmbda=0` on the artificial data set.\n",
    "blr = BinaryLogisticRegression(maxiter=1, lmbda=0)\n",
    "blr.fit(X, y)\n",
    "# Make predictions using the fitted `BinaryLogisticRegression` model on the training set `X`.\n",
    "y_pred = blr.predict(X)\n",
    "\n",
    "# Compute empirical risk as `risk` with the zero-one loss function on the training set `X`.\n",
    "risk = zero_one_loss(y, y_pred)\n",
    "\n",
    "assert risk == 0, 'The classifier must reach an accuracy of 1.0 for this data set.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5f6295",
   "metadata": {},
   "source": [
    "### **3. Nonlinear Basis Functions** <a class=\"anchor\" id=\"basis-functions\"></a>\n",
    "\n",
    "For certain learning tasks, it is useful to add complexity to a machine learning model by considering\n",
    "nonlinear transformations of the samples. \n",
    "\n",
    "**Definition 3.12** <font color='red'>**Nonlinear Basis Functions**</font> \n",
    "\n",
    "Nonlinear basis functions $\\boldsymbol{\\phi}: \\mathbb{R}^D \\rightarrow \\mathbb{R}^M$ nonlinearly transform a $D$-dimensional instance $\\mathbf{x} \\in \\mathbb{R}^D$ to an $M$-dimensional features space $\\mathbb{R}^M$ according to:\n",
    "\n",
    "$$\\boldsymbol{\\phi}(\\mathbf{x}) = \\begin{bmatrix} \\phi_1(\\mathbf{x}) \\\\ \\vdots \\\\ \\phi_M(\\mathbf{x}) \\end{bmatrix}.$$\n",
    "\n",
    "**Remark:** With nonlinear basis functions, a (generalized) linear machine learning model, e.g., BLR, can output nonlinear outputs, e.g., decision boundaries.\n",
    "\n",
    "**Examples:** Two popular types of nonlinear basis functions are: \n",
    "- *multivariate polynomials* with a user-defined maximum degree, e.g., for a degree of two and a two-dimensional instance, we get\n",
    "\n",
    "  $$\\boldsymbol{\\phi}(\\mathbf{x}) = \\boldsymbol{\\phi}\\left(\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}\\right) = \\begin{bmatrix} 1 \\\\ x_1 \\\\ x_2 \\\\ x_1 \\cdot x_2 \\\\ x_1^2 \\\\ x_2^2 \\end{bmatrix},$$\n",
    "  \n",
    "- and *Gaussian basis functions* of the form\n",
    "\n",
    "  $$\\phi_j(\\mathbf{x}) = \\exp\\left(\\frac{-||\\mathbf{x} - \\boldsymbol{\\mu}_j||_2^2}{2 \\sigma_j^2}\\right),$$\n",
    "\n",
    "  where $\\boldsymbol{\\mu} \\in \\mathbb{R}^D$ controls the position of the $j$-th basis function and the parameter $\\sigma_j \\in \\mathbb{R}_>0$ its spatial extension.\n",
    "  \n",
    "In the following, we exemplary illustrate the use of Gaussian basis functions. For this purpose, we want to define two basis functions such that the transformed instances define a linearly separable training set, i.e., it should be possible to draw a single straight line separating the blue from the red instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5155a49e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dea2c81b82e45dfa6f934f91457ab19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='mu_11', max=10.0, min=-10.0), FloatSlider(value=0.0,…"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "def visualize_gaussian_basis_functions(mu_11, mu_12, mu_21, mu_22, sigma_1, sigma_2):\n",
    "    \"\"\"\n",
    "    Visualizes the application of Gaussian basis functions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mu_11 : float\n",
    "        First value of mu_1 of the first basis function.\n",
    "    mu_12 : float\n",
    "        Second value of mu_1 of the first basis function.\n",
    "    mu_21 : float\n",
    "        First value of mu_2 of the second basis function.\n",
    "    mu_22 : float\n",
    "        Second value of mu_2 of the second basis function.\n",
    "    sigma_1 : positive float\n",
    "        Spatial extension of the first basis function.\n",
    "    sigma_2 : positive float\n",
    "        Spatial extension of the second basis function.\n",
    "    \"\"\"\n",
    "    X, y = make_blobs(n_samples=50, centers=4, random_state=7, cluster_std=0.75)\n",
    "    y %= 2\n",
    "    \n",
    "    # Define array of `mus`.\n",
    "    mus = np.array([[mu_11, mu_12], [mu_21, mu_22]])\n",
    "    \n",
    "    # Define array of `sigmas`.\n",
    "    sigmas = np.array([sigma_1, sigma_2])\n",
    "    \n",
    "    # Compute transformation via Gaussian basis functions.\n",
    "    Phi = np.array([[np.exp((-np.linalg.norm(x-mus[0])**2)/(2*sigmas[0]**2)), np.exp((-np.linalg.norm(x-mus[1])**2)/(2*sigmas[1]**2))] for x in X])\n",
    "    \n",
    "    # Plot results.\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', s=40, alpha=0.3)\n",
    "    plt.scatter(mus[:, 0], mus[:, 1], c=\"green\", s=120, marker=\"x\", label=\"$(\\mu_1, \\mu_2)^\\mathrm{T}$\")\n",
    "    plt.xlabel(\"$x_1$\")\n",
    "    plt.ylabel(\"$x_2$\")\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(Phi[:, 0], Phi[:, 1], c=y, cmap='coolwarm', s=40, alpha=0.3)\n",
    "    plt.xlabel(\"$\\phi_1$\")\n",
    "    plt.ylabel(\"$\\phi_2$\")\n",
    "    plt.show()\n",
    "    \n",
    "interactive(\n",
    "    visualize_gaussian_basis_functions, \n",
    "    mu_11=FloatSlider(value=0, min=-10, max=10),\n",
    "    mu_12=FloatSlider(value=0, min=-10, max=10),\n",
    "    mu_21=FloatSlider(value=0, min=-10, max=10),\n",
    "    mu_22=FloatSlider(value=0, min=-10, max=10),\n",
    "    sigma_1=FloatSlider(value=1, min=0.1, max=10),\n",
    "    sigma_2=FloatSlider(value=1, min=0.1, max=10),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58147427",
   "metadata": {},
   "source": [
    "#### **Questions:**\n",
    "3. (a) What is a suitable parametrization of the above basis functions for obtaining a linearly separable transformation?\n",
    "\n",
    "   mus = [[3, 0.7], [0, -1]]\n",
    "   sigmas = [7,7]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595067bd",
   "metadata": {},
   "source": [
    "### **4. Under- and Overfitting** <a class=\"anchor\" id=\"under-overfitting\"></a>\n",
    "\n",
    "For our two-dimensional data set, we want to visualize the BLR models estimated decision boundaries including the estimated class-membership probabilities.\n",
    "For this purpose, we implement the function [`plot_decision_boundary`](../e2ml/evaluation/_visualization.py) in the [`e2ml.evaluation`](../e2ml/evaluation) subpackage.\n",
    "\n",
    "Given this function, we create an interactive visualization in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5c3d40",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from e2ml.evaluation import plot_decision_boundary\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def visualize_blr(maxiter, lmbda, degree, train_ratio, random_state):\n",
    "    \"\"\"\n",
    "    Visualize decision boundary and estimated conditional class probabilities for a BLR model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    maxiter : positive int\n",
    "        Number of maximum optimization iterations.\n",
    "    lmbda : non-negative float\n",
    "        Regularization rate used by the RRM algorithm.\n",
    "    degree : non-negative int\n",
    "        Maximum degree of the multivariate polynomial.\n",
    "    train_ratio : float in (0, 1)\n",
    "        Ratio of training samples.\n",
    "    \"\"\"\n",
    "    # Generate dataset.\n",
    "    X, y = make_classification(\n",
    "        n_samples=100,\n",
    "        n_features=2,\n",
    "        n_redundant=0,\n",
    "        flip_y=0.25,\n",
    "        random_state=100,\n",
    "        scale=0.1,\n",
    "        class_sep=1.5,\n",
    "    )\n",
    "    \n",
    "    # Get boundaries of the feature space.\n",
    "    bound=[[X[:, 0].min()-0.2, X[:, 1].min()-0.2], [X[:, 0].max()+0.2, X[:, 1].max()+0.2]]\n",
    "    \n",
    "    # Randomly split train and test data by creating the index arrays `train_idx` and `test_idx`.\n",
    "    # TODO\n",
    "    \n",
    "    # Create polynomial feature transformation `poly` with `degree`.\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    \n",
    "    # Create `blr` model with `n_optimization_steps` and `lmbda`.\n",
    "    # TODO\n",
    "    \n",
    "    # Create pipeline.\n",
    "    clf = make_pipeline(poly, blr)\n",
    "    \n",
    "    # Fit `BinaryLogisticRegression` as `clf` on training data.\n",
    "    # TODO\n",
    "    \n",
    "    # Compute zero-one loss on train and test data, i.e., `train_error` and `test_error`.\n",
    "    # TODO\n",
    "    \n",
    "    # Plot results.\n",
    "    plot_decision_boundary(clf=clf, bound=bound)\n",
    "    plt.scatter(\n",
    "        X[train_idx, 0],\n",
    "        X[train_idx, 1],\n",
    "        marker='o',\n",
    "        label='Training Samples',\n",
    "        facecolors='none',\n",
    "        edgecolors='k',\n",
    "        linewidth=4, \n",
    "        s=60\n",
    "    )\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', s=40)\n",
    "    plt.title(f'Train Error: {round(train_error, 3)}, Test Error: {round(test_error, 3)}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "interactive(\n",
    "    visualize_blr, \n",
    "    maxiter=IntSlider(value=1000, min=0, max=10000), \n",
    "    eta=FloatSlider(value=1, min=0, max=10), \n",
    "    lmbda=FloatSlider(value=0.0, min=0.0, max=10, step=0.001),\n",
    "    degree=IntSlider(value=1, min=1, max=10), \n",
    "    train_ratio=FloatSlider(value=0.5, min=0.02, max=0.98, step=0.01),\n",
    "    random_state=IntSlider(value=0, min=0, max=10),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a7363d",
   "metadata": {},
   "source": [
    "The interactive visualization allows us to study the effect of the two parameters `lmbda` and `degree`. In particular, we can study the phonemes of under- and overfitting to answer the following questions:\n",
    "\n",
    "#### Questions:\n",
    "4. (a) How does the regularization rate `lmbda` affect the BLR model's decision boundary and its estimated class-membership probabilities? How is it related to over- and underfitting?\n",
    "\n",
    "   TODO\n",
    "   \n",
    "  (b) How does the `degree` of the polynomial affect the BLR model's decision boundary and its estimated class-membership probabilities? How is it related to over- and underfitting?\n",
    "\n",
    "   TODO\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e2ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
