{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c70fa63-b796-4b94-b670-0e98bb66a5a1",
   "metadata": {},
   "source": [
    "# Final Excercise\n",
    "\n",
    "In this notebook, you will find the last exercise of the lecture:\n",
    "You are handed an initial dataset with several features and a univariate target. Next, you have to decide how to proceed. Since you do not have enough data to yet construct a classifier/predictor of any sensible evaluation metrics, the first task is, therefore, to acquire more data. For this purpose you can obtain batches of data according to your own design of experiments, so you will need to decide which experiments you consider necessary to perform. \n",
    "\n",
    "You will have four opportunities to acquire more data. Each time you have to decide which experiments to run and send those to Franz Götz-Hahn as a CSV file. The deadlines are 16.06.2023, 23.06.2023, 30.06.2023, and 07.07.2023 and 12:00 (noon). The format in all cases is a table with one row for each choosable feature, and the column entries corresponding to the desired values. Each individual sample will take approximately 30min, so pick a reasonable amount of experiments. For example, you will get the result for 100 experiments roughly 50 hours after the respective deadline. Should the experiment not be conductible, you will get a ``None`` as a result, e.g., if a feature value is out of range.\n",
    "\n",
    "Once you have your data, you should compare the performance of different classifiers in predicting the targets. The classifiers to compare are [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC), and [MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html). You should utilize all the different parts of the E2ML lecture that you considern appropriate. This could include Data Preprocessing, Design of Experiments for the batches, deciding on Performance Measures, Statistical Significance Testing of a hypothesis, Design of Experiments for Hyperparameter Optimization.\n",
    "\n",
    "Should you wish to present the results from this excercise in the oral examination, you need to hand in your entire package until 14.07.2023-23:59 as a GitHub Repository. Send the link to the (public) repository to Franz Götz-Hahn via [E-Mail](mailto:franz.goetz-hahn@uni-kassel.de). Please use the README of the repository to describe the structure of the package, include any required packages in the setup.py, add the data in the data subfolder, save any results in the results subfolder, and include a _descriptive_ jupyter notebook in the notebooks subfolder.\n",
    "\n",
    "Do note, that the point of this excercise is **not** to achieve the best performance of your models, but rather to document your process and give the motivation behind your chosen approaches, _even the ones that failed_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cbb5690-6d49-42df-bf4b-db120f76c893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import log_loss\n",
    "from e2ml.experimentation import perform_bayesian_optimization\n",
    "from e2ml.preprocessing import PrincipalComponentAnalysis\n",
    "from sklearn.decomposition import PCA# used because at the time of writing this code the solutions weren't available and I wanted to make sure the code is right\n",
    "from e2ml.experimentation import acquisition_ei\n",
    "from e2ml.models import GaussianProcessRegression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8e495a7",
   "metadata": {},
   "source": [
    "### **Mollusc Classification** <a class=\"anchor\" id=\"heart\"></a>\n",
    "\n",
    "Your dataset describes some physical measurements of a specific type of molluscs. Your goal is to predict the `Stage of Life` of the mollusc. The data you can get looks as follows:\n",
    "\n",
    "\n",
    "| Sex\t|Length\t|Width\t|Height|\tWeight\t|Non_Shell Weight\t|Intestine Weight\t|Shell Weight\t|Stage of Life |\n",
    "| ---                           | ----   | ----    | ----    | ----   |----             |----    |---- | ---------- |\n",
    "| {Male (M), Female (F), Indeterminate (I)} | float (inches)     | float (inches)     |  float (inches)     | float (gram)      | float (gram)              | float (gram)     |  float (gram)     | {Child, Adolescent (Adole), Adult}      |\n",
    "\n",
    "The table headings are identical to the column names in the corresponding CSV-files. \n",
    "\n",
    "We can send out divers that look for molluscs that fit your needs, which will subsequently be analyzed in a laboratory. You can request molluscs with all features except the Stage of Life attribute, as it is the target. The first day of diving has already been completed. After 8 hours of diving, they brought up the following molluscs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8435dbee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Length</th>\n",
       "      <th>Width</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Non_Shell Weight</th>\n",
       "      <th>Intestine Weight</th>\n",
       "      <th>Shell Weight</th>\n",
       "      <th>Stage of Life</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.4165</td>\n",
       "      <td>0.1655</td>\n",
       "      <td>0.0950</td>\n",
       "      <td>0.1350</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.5700</td>\n",
       "      <td>0.1670</td>\n",
       "      <td>0.1180</td>\n",
       "      <td>0.1870</td>\n",
       "      <td>Adole</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.170</td>\n",
       "      <td>1.0225</td>\n",
       "      <td>0.4190</td>\n",
       "      <td>0.2405</td>\n",
       "      <td>0.3600</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.4405</td>\n",
       "      <td>0.2385</td>\n",
       "      <td>0.0745</td>\n",
       "      <td>0.1075</td>\n",
       "      <td>Adole</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.0425</td>\n",
       "      <td>0.0170</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.0155</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.175</td>\n",
       "      <td>1.3445</td>\n",
       "      <td>0.5490</td>\n",
       "      <td>0.2875</td>\n",
       "      <td>0.3600</td>\n",
       "      <td>Child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I</td>\n",
       "      <td>0.515</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.5310</td>\n",
       "      <td>0.2415</td>\n",
       "      <td>0.0980</td>\n",
       "      <td>0.1615</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.160</td>\n",
       "      <td>1.1115</td>\n",
       "      <td>0.4495</td>\n",
       "      <td>0.2825</td>\n",
       "      <td>0.3450</td>\n",
       "      <td>Child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.195</td>\n",
       "      <td>1.6275</td>\n",
       "      <td>0.6890</td>\n",
       "      <td>0.3905</td>\n",
       "      <td>0.4320</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.165</td>\n",
       "      <td>1.0430</td>\n",
       "      <td>0.4835</td>\n",
       "      <td>0.2210</td>\n",
       "      <td>0.3100</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.8075</td>\n",
       "      <td>0.3220</td>\n",
       "      <td>0.1810</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>Adole</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.1025</td>\n",
       "      <td>0.0420</td>\n",
       "      <td>0.0655</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>F</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.195</td>\n",
       "      <td>1.7775</td>\n",
       "      <td>0.8610</td>\n",
       "      <td>0.3220</td>\n",
       "      <td>0.4150</td>\n",
       "      <td>Adole</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>F</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.6985</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.1460</td>\n",
       "      <td>0.1950</td>\n",
       "      <td>Adole</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>M</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.155</td>\n",
       "      <td>1.0650</td>\n",
       "      <td>0.4860</td>\n",
       "      <td>0.2330</td>\n",
       "      <td>0.2850</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>M</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.185</td>\n",
       "      <td>1.3200</td>\n",
       "      <td>0.5305</td>\n",
       "      <td>0.2635</td>\n",
       "      <td>0.4550</td>\n",
       "      <td>Child</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sex  Length  Width  Height  Weight  Non_Shell Weight  Intestine Weight  \\\n",
       "0    F   0.450  0.345   0.120  0.4165            0.1655            0.0950   \n",
       "1    F   0.475  0.380   0.145  0.5700            0.1670            0.1180   \n",
       "2    M   0.610  0.485   0.170  1.0225            0.4190            0.2405   \n",
       "3    I   0.430  0.340   0.105  0.4405            0.2385            0.0745   \n",
       "4    M   0.205  0.155   0.045  0.0425            0.0170            0.0055   \n",
       "5    M   0.600  0.475   0.175  1.3445            0.5490            0.2875   \n",
       "6    I   0.515  0.390   0.110  0.5310            0.2415            0.0980   \n",
       "7    F   0.625  0.495   0.160  1.1115            0.4495            0.2825   \n",
       "8    F   0.650  0.520   0.195  1.6275            0.6890            0.3905   \n",
       "9    F   0.620  0.480   0.165  1.0430            0.4835            0.2210   \n",
       "10   F   0.535  0.450   0.135  0.8075            0.3220            0.1810   \n",
       "11   I   0.385  0.280   0.090  0.2280            0.1025            0.0420   \n",
       "12   F   0.680  0.560   0.195  1.7775            0.8610            0.3220   \n",
       "13   F   0.585  0.415   0.155  0.6985            0.3000            0.1460   \n",
       "14   M   0.525  0.435   0.155  1.0650            0.4860            0.2330   \n",
       "15   M   0.660  0.545   0.185  1.3200            0.5305            0.2635   \n",
       "\n",
       "    Shell Weight Stage of Life  \n",
       "0         0.1350         Adult  \n",
       "1         0.1870         Adole  \n",
       "2         0.3600         Adult  \n",
       "3         0.1075         Adole  \n",
       "4         0.0155         Adult  \n",
       "5         0.3600         Child  \n",
       "6         0.1615         Adult  \n",
       "7         0.3450         Child  \n",
       "8         0.4320         Adult  \n",
       "9         0.3100         Adult  \n",
       "10        0.2500         Adole  \n",
       "11        0.0655         Adult  \n",
       "12        0.4150         Adole  \n",
       "13        0.1950         Adole  \n",
       "14        0.2850         Adult  \n",
       "15        0.4550         Child  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_molluscs_data = pd.read_csv('../data/initial_molluscs_data.csv')\n",
    "initial_molluscs_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6baa2d",
   "metadata": {},
   "source": [
    "## **This Notebook only contains the first prototyping and creation of the first batch of data, it is not pretty but it is what I initally did so i kept it in. For the actual code I used to get the third to last batch look into new_data_pipeline.ipynb**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46664260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp\n",
      "predicted: [[0.42943023 0.37444436 0.19612541]\n",
      " [0.20951428 0.56118949 0.22929623]\n",
      " [0.29903595 0.43849029 0.26247376]\n",
      " [0.24140385 0.44460473 0.31399142]\n",
      " [0.21586358 0.31799939 0.46613703]] real: [[0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "svc\n",
      "predicted: [[0.40766389 0.27408039 0.31825572]\n",
      " [0.39480692 0.32064965 0.28454343]\n",
      " [0.39655424 0.29753548 0.30591028]\n",
      " [0.40175276 0.32239626 0.27585098]\n",
      " [0.40758887 0.3233908  0.26902032]] real: [[0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "rfc\n",
      "predicted: [[0.39134195 0.37599722 0.23266083]\n",
      " [0.43409497 0.23119542 0.33470961]\n",
      " [0.4298035  0.3121014  0.2580951 ]\n",
      " [0.45558268 0.24507817 0.29933915]\n",
      " [0.38365173 0.23269654 0.38365173]] real: [[0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ben-g\\anaconda3\\envs\\e2ml-env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#group data by Stage of Life\n",
    "adults = initial_molluscs_data.loc[initial_molluscs_data[\"Stage of Life\"] == \"Adult\"]\n",
    "adoles = initial_molluscs_data.loc[initial_molluscs_data[\"Stage of Life\"] == \"Adole\"]\n",
    "children = initial_molluscs_data.loc[initial_molluscs_data[\"Stage of Life\"] == \"Child\"]\n",
    "# just get the single columns\n",
    "full_length = initial_molluscs_data[\"Length\"]\n",
    "full_width = initial_molluscs_data[\"Width\"]\n",
    "full_height = initial_molluscs_data[\"Height\"]\n",
    "full_weight = initial_molluscs_data[\"Weight\"]\n",
    "full_non_shell_weight = initial_molluscs_data[\"Non_Shell Weight\"]\n",
    "full_intestine_weight = initial_molluscs_data[\"Intestine Weight\"]\n",
    "full_shell_weight = initial_molluscs_data[\"Shell Weight\"]\n",
    "full_sexes = initial_molluscs_data[\"Sex\"]\n",
    "full_stages = initial_molluscs_data[\"Stage of Life\"]\n",
    "\n",
    "# compute statistics about the relation of different features\n",
    "volume = full_length * full_width * full_height\n",
    "weight_volume_quotients = full_weight / volume \n",
    "non_shell_quotient = full_non_shell_weight / full_weight\n",
    "intestine_quotient = full_intestine_weight / full_weight\n",
    "shell_quotient = full_shell_weight / full_weight\n",
    "\n",
    "def getOneHotEncoding(data):\n",
    "    values = np.sort(np.unique(data))\n",
    "    enc = np.zeros((len(data), len(values)))\n",
    "    for i, x in enumerate(data):\n",
    "        enc[i, np.where(values == x)[0][0]] = 1\n",
    "    return enc\n",
    "\n",
    "def score_cross_entropy_loss(mdl, x, y):\n",
    "    if(len(y.shape) == 1):\n",
    "        y = getOneHotEncoding(y)\n",
    "    y_pred = softmax(mdl.predict_proba(x))\n",
    "    #return cross_entropy_loss(y, y_pred)\n",
    "    return [log_loss(y[i], y_pred[i])*-1 for i in range(len(y_pred))]\n",
    "\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    #if y contains the information of which class is true as index\n",
    "    if(len(y_true.shape) == 1):\n",
    "        return np.array([np.log(y_pred[i]) for i in y_true])\n",
    "    #if onehot encoded\n",
    "    elif(len(y_true.shape) == 2):\n",
    "        return np.array([-(y_true[i] * np.log(y_pred[i])).sum() for i in range(len(y_pred))])\n",
    "    \n",
    "def softmax(x):\n",
    "    x = np.array(x)\n",
    "    return np.exp(x) / np.exp(x).sum(axis=1).reshape(x.shape[0],-1)\n",
    "\n",
    "\"\"\"\n",
    "get a first Idea of how the Models perform on the inital data (at this time I didn't know that we would get results in the email as well)\n",
    "\"\"\"\n",
    "#Encode sexes using one-hot encoding and create training data and labels\n",
    "x = np.concatenate((getOneHotEncoding(full_sexes), initial_molluscs_data.values[:,1:-1]), axis=1)\n",
    "y = getOneHotEncoding(full_stages)\n",
    "\n",
    "print(\"mlp\")\n",
    "mlp = MLPClassifier(max_iter=1000)\n",
    "mlp.fit(x[:10], y[:10])\n",
    "y_pred = softmax(mlp.predict_proba(x[11:]))\n",
    "print(f\"predicted: {y_pred} real: {y[11:]}\")\n",
    "mlp = MLPClassifier(max_iter=1000)\n",
    "#see how good the model performs using cross entropy as the metric (code doesn't work anymore and i'm too lazy to see why and fix it, I hope the idea that is presented is enough)\n",
    "#cvs_mlp = cross_val_score(mlp, x, y, cv=3, scoring=score_cross_entropy_loss)\n",
    "#print(cvs_mlp)\n",
    "\n",
    "#encode labels as numbers for SVC and RFC\n",
    "y_rfc = full_stages.replace(\"Adult\",0).replace(\"Adole\",1).replace(\"Child\",2)\n",
    "\n",
    "print(\"svc\")\n",
    "svc = SVC(kernel=\"rbf\", probability=True)\n",
    "svc.fit(x[:10], y_rfc[:10])\n",
    "y_pred = softmax(svc.predict_proba(x[11:]))\n",
    "print(f\"predicted: {y_pred} real: {y[11:]}\")\n",
    "svc = SVC(kernel=\"rbf\", probability=True)\n",
    "#see how good the model performs using cross entropy as the metric\n",
    "#cvs_svc = cross_val_score(svc, x, y_rfc, cv=3, scoring=score_cross_entropy_loss)\n",
    "#print(cvs_svc)\n",
    "\n",
    "print(\"rfc\")\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(x[:10], y_rfc[:10])\n",
    "y_pred = softmax(rfc.predict_proba(x[11:]))\n",
    "print(f\"predicted: {y_pred} real: {y[11:]}\")\n",
    "rfc = RandomForestClassifier()\n",
    "#see how good the model performs using cross entropy as the metric\n",
    "#cvs_rfc = cross_val_score(rfc, x, y_rfc, cv=3, scoring=score_cross_entropy_loss)\n",
    "#print(f\"{cvs_rfc=}\")\n",
    "\n",
    "#First idea for an objective function for baysian optimization\n",
    "def objectiveFunction(x, y):\n",
    "    rfc = RandomForestClassifier()\n",
    "    cvs_rfc = cross_val_score(rfc, x, y_rfc, cv=3, scoring=score_cross_entropy_loss)\n",
    "    svc = SVC(kernel=\"rbf\", probability=True)\n",
    "    cvs_svc = cross_val_score(svc, x, y_rfc, cv=3, scoring=score_cross_entropy_loss)\n",
    "    mlp = MLPClassifier(max_iter=1000)\n",
    "    cvs_mlp = cross_val_score(mlp, x, y, cv=3, scoring=score_cross_entropy_loss)\n",
    "    return (cvs_rfc.mean() + cvs_svc.mean() + cvs_mlp.mean()) / 3\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594250e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get the first batch of data.\n",
    "Since time was close and I needed to get data, I opted to just draw samples randomly.\n",
    "Sexes were choses from the original distribution by drawing with placing back.\n",
    "Here I made the assumption that the volume and the weight of the mollusc are dependent on each other, and I could calculate the weight by multiplying the weigth by a quotient with some noise.\n",
    "To achieve this I fitted a normal distribution with the qutient of the molluscs I got from the inital data.\n",
    "I also made the assumption that the volume and thus the length, width and height would be normaly distributed as often time in nature this is the case (height for humans for example).\n",
    "I used the inital data to parameterize these normal distributions and then drew samples from them to get the data for the first batch.\n",
    "I also checked the mean and standard deviation of the new data and some extra statistics to see if it would actually match those of the old data.\n",
    "\"\"\"\n",
    "\n",
    "def getNewSamples(old_data:pd.Series, size:int):\n",
    "    return np.random.normal(old_data.mean(), scale=old_data.std(), size=size)\n",
    "\n",
    "def getDict(size:int):\n",
    "    \"\"\"\n",
    "    Just get a dict with the values that can be turned into a dataframe using pandas\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    d[\"Sex\"] = np.random.choice(full_sexes, size)\n",
    "    d[\"Length\"] = getNewSamples(full_length, size)\n",
    "    d[\"Width\"] = getNewSamples(full_width, size)\n",
    "    d[\"Height\"] =  getNewSamples(full_height, size)\n",
    "    d[\"Weight\"] = d[\"Height\"] * d[\"Width\"] * d[\"Length\"] * np.random.normal(weight_volume_quotients.mean(), weight_volume_quotients.std())\n",
    "    d[\"Non_Shell Weight\"] = d[\"Weight\"] * np.random.normal(non_shell_quotient.mean(), non_shell_quotient.std())\n",
    "    d[\"Intestine Weight\"] = d[\"Weight\"] * np.random.normal(intestine_quotient.mean(), intestine_quotient.std())\n",
    "    d[\"Shell Weight\"] = d[\"Weight\"] * np.random.normal(shell_quotient.mean(), shell_quotient.std())\n",
    "    return d\n",
    "d = getDict(282)\n",
    "print(f\"{d['Weight'].mean()=} {d['Weight'].std()=}\")\n",
    "print(f\"{full_weight.mean()=} {full_weight.std()=}\")\n",
    "print(f\"{d['Non_Shell Weight'].mean()=} {d['Non_Shell Weight'].std()=}\")\n",
    "print(f\"{full_non_shell_weight.mean()=} {full_non_shell_weight.std()=}\")\n",
    "print(f\"{d['Intestine Weight'].mean()=} {d['Intestine Weight'].std()=}\")\n",
    "print(f\"{full_intestine_weight.mean()=} {full_intestine_weight.std()=}\")\n",
    "print(f\"{d['Shell Weight'].mean()=} {d['Shell Weight'].std()=}\")\n",
    "print(f\"{full_shell_weight.mean()=} {full_shell_weight.std()=}\")\n",
    "orignial_diffs = full_weight - full_intestine_weight - full_non_shell_weight - full_shell_weight\n",
    "new_diffs = d[\"Weight\"] - d[\"Intestine Weight\"] - d[\"Non_Shell Weight\"] - d[\"Shell Weight\"]\n",
    "print(orignial_diffs)\n",
    "print(new_diffs)\n",
    "print(f\"{orignial_diffs.mean()=} {orignial_diffs.std()=}\")\n",
    "print(f\"{new_diffs.mean()=} {new_diffs.std()=}\")\n",
    "print(pd.DataFrame(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3bac9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Unnamed: 0 Sex    Length     Width    Height    Weight  Non_Shell Weight  \\\n",
      "0             0   M  0.471305  0.290373  0.106767  0.303367          0.123269   \n",
      "1             1   F  0.731545  0.478463  0.170384  1.238197          0.503123   \n",
      "2             2   I  0.615436  0.235762  0.065890  0.198494          0.080655   \n",
      "3             3   F  0.451397  0.144832  0.161660  0.219429          0.089162   \n",
      "4             4   F  0.689066  0.398501  0.175774  1.002112          0.407194   \n",
      "..          ...  ..       ...       ...       ...       ...               ...   \n",
      "277         277   F  0.555961  0.375405  0.131120  0.568176          0.230870   \n",
      "278         278   F  0.605268  0.583672  0.065907  0.483415          0.196429   \n",
      "279         279   M  0.529225  0.241043  0.140752  0.372786          0.151476   \n",
      "280         280   F  0.411005  0.499416  0.121998  0.519914          0.211260   \n",
      "281         281   M  0.603832  0.480075  0.120224  0.723582          0.294017   \n",
      "\n",
      "     Intestine Weight  Shell Weight  \n",
      "0            0.060426      0.095332  \n",
      "1            0.246630      0.389098  \n",
      "2            0.039537      0.062376  \n",
      "3            0.043707      0.068955  \n",
      "4            0.199605      0.314909  \n",
      "..                ...           ...  \n",
      "277          0.113172      0.178547  \n",
      "278          0.096289      0.151911  \n",
      "279          0.074253      0.117146  \n",
      "280          0.103559      0.163381  \n",
      "281          0.144126      0.227382  \n",
      "\n",
      "[282 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "print(pd.read_csv(\"FirstBatchBen.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f270c710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.70537116 0.28817022]\n",
      "[3.54073097 2.26312547]\n",
      "[[-0.74840581 -0.62740424]\n",
      " [-0.76548386 -0.47050949]\n",
      " [ 1.15757455  0.32628467]\n",
      " [ 0.24195993 -0.4856955 ]\n",
      " [ 1.29797218 -0.90029362]\n",
      " [ 1.12356131  0.64986098]\n",
      " [ 0.22758857 -0.36834205]\n",
      " [-0.84014791  0.2007329 ]\n",
      " [-0.90098976  0.77220098]\n",
      " [-0.83319121  0.13402528]\n",
      " [-0.79940449 -0.16651914]\n",
      " [ 0.2713473  -0.7486818 ]\n",
      " [-0.92209266  0.9578332 ]\n",
      " [-0.78875156 -0.27996054]\n",
      " [ 1.15739731  0.34398507]\n",
      " [ 1.12106611  0.66248328]]\n",
      "mlp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ben-g\\anaconda3\\envs\\e2ml-env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svc\n",
      "rfc\n",
      "[-0.4773645723611691, -0.4098049359074743, -0.3729031519548653, -0.530239431883584, -0.35372802568051775, -0.39291368919781977, -0.4762058457221013, -0.5027245280704169, -0.47696637052782237, -0.4979112796885527, -0.37696525203047004, -0.40095331281649926, -0.4731118341069962, -0.3690954346579947, -0.3729031519548653, -0.39291368919781977]\n",
      "[-0.515057829735693, -0.7002840914250089, -0.6009751991975496, -0.5760999613352049, -0.6257006415990883, -0.658015775472558, -0.6171860047097263, -0.7230883435484797, -0.5740028297436728, -0.5296547225791984, -0.6975862318597081, -0.6279255232347337, -0.6186053714378043, -0.7013980834181033, -0.5998145419708573, -0.6585480373443983]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ben-g\\anaconda3\\envs\\e2ml-env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.5164647053346821, -0.47450517476205495, -0.3590114280451305, -0.6232765364671853, -0.34353638415286, -0.36449831160723517, -0.530208813762346, -0.5979227025085164, -0.501828882620654, -0.5810820576470737, -0.43785968140227904, -0.3850810852009933, -0.4856460443119528, -0.38861726717627904, -0.36477844752264693, -0.3606014681718168]\n",
      "(10000, 2)\n",
      "[0.60069514 0.60069514 0.60069514 ... 0.60069514 0.60069514 0.60069514]\n",
      "0.5703279326726639\n",
      "0.08902937074562546\n",
      "6.890552061449001e-06\n",
      "0.6006951355493214\n",
      "[0.60069514 0.60069514 0.60069514 0.60069514 0.60069514 0.60069514\n",
      " 0.60069514 0.60069514 0.60069514 0.60069514]\n",
      "[[0.13636364 1.04545455]\n",
      " [0.28787879 1.01515152]\n",
      " [0.22727273 1.01515152]\n",
      " [0.1969697  1.01515152]\n",
      " [0.16666667 1.01515152]\n",
      " [0.13636364 1.01515152]\n",
      " [0.10606061 1.01515152]\n",
      " [0.07575758 1.01515152]\n",
      " [0.25757576 1.01515152]\n",
      " [1.5        1.5       ]]\n",
      "[[1.06268547 0.72025198 0.58817507 0.20941003 1.74205705 0.7599597\n",
      "  0.37273336 0.48780996]\n",
      " [1.20990022 0.70836333 0.57865376 0.20571893 1.70334774 0.74240059\n",
      "  0.36406476 0.4791247 ]\n",
      " [1.14968446 0.71089758 0.58048636 0.20641945 1.70867304 0.74491102\n",
      "  0.36535137 0.47987915]\n",
      " [1.11957659 0.71216471 0.58140266 0.2067697  1.71133569 0.74616624\n",
      "  0.36599467 0.48025638]\n",
      " [1.08946871 0.71343184 0.58231896 0.20711996 1.71399834 0.74742146\n",
      "  0.36663797 0.48063361]\n",
      " [1.05936083 0.71469896 0.58323526 0.20747022 1.71666099 0.74867667\n",
      "  0.36728127 0.48101083]\n",
      " [1.02925295 0.71596609 0.58415157 0.20782047 1.71932364 0.74993189\n",
      "  0.36792458 0.48138806]\n",
      " [0.99914507 0.71723322 0.58506787 0.20817073 1.72198629 0.75118711\n",
      "  0.36856788 0.48176529]\n",
      " [1.17979234 0.70963046 0.57957006 0.20606919 1.70601039 0.7436558\n",
      "  0.36470807 0.47950192]\n",
      " [2.46740956 0.74652652 0.62103865 0.22274574 2.00317862 0.87272031\n",
      "  0.42556606 0.57282169]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "My idea for getting new data from the beginning was to use baysian optimization (BO).\n",
    "The Idea is using cross_entropy as a metric on how well the model performs and use that to perform BO.\n",
    "The first big problem I encountered was the problem of dimensionality.\n",
    "It was not possible to use the original feature space with all 7 dimensions to create a good amount of candidates.\n",
    "For example if I took only 100 possible values for each feature I would get 100^8 = 1e16 possible combinations (about 10 Petaabyte if float32 used and my math is not off).\n",
    "That wouldn't fit into my ram. I could reduce the number possible values but that would limit the range of samples I could get drasticly and make it hard for the BO to actually find good samples for the next batch of data\n",
    "To solve that problem I took a look into how good PCA works for dimensionality reduction.\n",
    "The first 2 prinicpled components were responsible for 88% of the variance in the dataset, so pca looked like a good choice to reduce the dimensionality to a good amount for BO and still keep most of the information present in the data.\n",
    "It also made sense to me, that pca would perform well, as I already had the idea that since most of the features were dependent on each other (full weight ~ Non_Shell Weight + Intestine Weight + Shell Weight, width*length*height ~ weight) a lot of varaince should be explainable by a lower number of features of some form.\n",
    "Since pca doesn't work on categorical data (sex) the sexes are represented as 0, 1, 2 and included in this way here. Good idea? We will see...\n",
    "\"\"\"\n",
    "\n",
    "full_replaced = initial_molluscs_data.replace(\"F\",0).replace(\"I\",1).replace(\"M\",2)\n",
    "x_full_replaced = full_replaced.values[:,:-1]\n",
    "pca = PCA(2)\n",
    "pca = pca.fit(x_full_replaced)\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.singular_values_)\n",
    "\n",
    "#look at the range of the values of the pca to determine the range of the candidates. In this case the range is (-1.5, 1.5)\n",
    "x_pca = pca.transform(x_full_replaced)\n",
    "print(x_pca)\n",
    "\n",
    "#new valuerange is -1.5, 1.5\n",
    "\n",
    "#look at performance of models if using the pca values\n",
    "print(\"mlp\")\n",
    "mlp = MLPClassifier(max_iter=1000)\n",
    "mlp.fit(x_pca[:10], y[:10])\n",
    "y_pred = mlp.predict_proba(x_pca[11:])\n",
    "y_pred = softmax(y_pred)\n",
    "mlp = MLPClassifier(max_iter=1000)\n",
    "#cvs_mlp = cross_val_score(mlp, x_pca, y, cv=3, scoring=score_cross_entropy_loss)\n",
    "#print(cvs_mlp)\n",
    "\n",
    "y_rfc = full_stages.replace(\"Adult\",0).replace(\"Adole\",1).replace(\"Child\",2)\n",
    "\n",
    "print(\"svc\")\n",
    "svc = SVC(kernel=\"rbf\", probability=True)\n",
    "svc.fit(x_pca[:10], y_rfc[:10])\n",
    "y_pred = softmax(svc.predict_proba(x_pca[11:]))\n",
    "svc = SVC(kernel=\"rbf\", probability=True)\n",
    "#cvs_svc = cross_val_score(svc, x_pca, y_rfc, cv=3, scoring=score_cross_entropy_loss)\n",
    "#print(cvs_svc)\n",
    "\n",
    "print(\"rfc\")\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(x_pca[:10], y_rfc[:10])\n",
    "y_pred = softmax(rfc.predict_proba(x_pca[11:]))\n",
    "rfc = RandomForestClassifier()\n",
    "#cvs_rfc = cross_val_score(rfc, x_pca, y_rfc, cv=3, scoring=score_cross_entropy_loss)\n",
    "#print(f\"{cvs_rfc=}\")\n",
    "\n",
    "def objectiveFunctionRFC(x, y_rfc):\n",
    "    rfc = RandomForestClassifier()\n",
    "    rfc.fit(x,y_rfc)\n",
    "    return score_cross_entropy_loss(rfc, x, y_rfc)\n",
    "\n",
    "def objectiveFunctionSVC(x,y_rfc):\n",
    "    svc = SVC(kernel=\"rbf\", probability=True)\n",
    "    svc.fit(x, y_rfc)\n",
    "    return score_cross_entropy_loss(svc, x, y_rfc)\n",
    "\n",
    "def objectiveFunctionMLP(x, y):\n",
    "    mlp = MLPClassifier(max_iter=1000)\n",
    "    mlp.fit(x,y)\n",
    "    return score_cross_entropy_loss(mlp, x, y)\n",
    "\n",
    "\n",
    "x_acquired = x_pca\n",
    "y_acquired_rfc = objectiveFunctionRFC(x_acquired, y_rfc)\n",
    "print(y_acquired_rfc)\n",
    "y_acquired_svc = objectiveFunctionSVC(x_pca, y_rfc)\n",
    "print(y_acquired_svc)\n",
    "y_acquired_mlp = objectiveFunctionMLP(x_pca, y)\n",
    "print(y_acquired_mlp)\n",
    "\n",
    "metrics_dict = {'gamma': 50, 'metric': 'rbf'}\n",
    "gpr = GaussianProcessRegression(metrics_dict=metrics_dict)\n",
    "gpr.fit(x_acquired, y_acquired_rfc)\n",
    "\n",
    "x1_new = np.linspace(-1.5, 1.5, 100)\n",
    "x_mesh,y_mesh = np.meshgrid(x1_new, x1_new)\n",
    "\n",
    "x_cand = np.stack((x_mesh, y_mesh), axis=2).reshape(-1,2)\n",
    "print(x_cand.shape)\n",
    "means, stds = gpr.predict(x_cand, True)\n",
    "scores = acquisition_ei(means, stds, max(y_acquired_rfc))\n",
    "print(scores)\n",
    "print(scores.mean())\n",
    "print(scores.std())\n",
    "print(scores.min())\n",
    "print(scores.max())\n",
    "\n",
    "nextidx = np.argsort(scores)\n",
    "\n",
    "print(scores[nextidx[-10:]])\n",
    "print(x_cand[nextidx[-10:]])\n",
    "print(pca.inverse_transform(x_cand[nextidx[-10:]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
