{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Notebook for the Hyperparamter optimization**\n",
    "I just use bayesian optimization (BO) (again ^^).\n",
    "I encode the hyperparameters into one array (hyperparameters) and call the optimization function just with that.\n",
    "As metric I use cross-entropy again, but this time actually the one for more than 2 classes.\n",
    "In the optimization function I use 5-fold cross-validation and avarage the cross-entropy-scores I get as the score to be optimized.\n",
    "Since our bayesian-optimization maximises the funtions, I multipy the score by -1 so the score gets minimized.\n",
    "Candidate values for the BO are just all combinations of hyperparameters.\n",
    "Thats kinda it.\n",
    "Results for me were lowest score: 0.58042515 best parameters: 2 hiddenlayers 33 hiddenunits alpha: 1e-03 learningrate: 1e-02 (see below for all, took ~1Hour)\n",
    "Scores were actually more spread than I would have thought but on the good side many Models performed relativly equal.\n",
    "The problem here is probably again the dataset and the very high portion of adults.\n",
    "For a higher number of evals the results would be more meaningful.\n",
    "\n",
    "## **Hypothesis test for Modelselection**\n",
    "I would have also performed a hypothesis test to see of one model is better than another.\n",
    "Problems I saw here were that comparing more than 2 models would need more sophisticated tests than the ones we implemented (Friedman -> Nemenyi for example).\n",
    "In the case of the Friedman test the distribution is only close to the chi squared distribution if the number of models that are compared is greater than 5 and more than 5 datasets are used (Statistical Comparisons of Classifiers over Multiple Data Sets, Janez Demsar).\n",
    "For lower numbers there are special values computed but I think this would be out of the scope of this project.\n",
    "Another (smaller) Problem is the size of the dataset (1480 samples).\n",
    "Splitting the dataset to aquire a meaningful number of measurements (>5) of the performance would probably signifcantly hurt the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.45       0.345      0.12       ... 0.1655     0.095      0.135     ]\n",
      " [0.475      0.38       0.145      ... 0.167      0.118      0.187     ]\n",
      " [0.61       0.485      0.17       ... 0.419      0.2405     0.36      ]\n",
      " ...\n",
      " [0.64668016 0.4351138  0.25650057 ... 0.58246501 0.28639193 0.44137179]\n",
      " [0.64307546 0.43226801 0.25525184 ... 0.57601198 0.28321823 0.43654118]\n",
      " [0.63947075 0.42942222 0.25400312 ... 0.56955895 0.28004452 0.43171058]]\n",
      "1480\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from e2ml.models._gaussian_process_regression_solution import GaussianProcessRegression\n",
    "from e2ml.experimentation._bayesian_optimization_solution import perform_bayesian_optimization\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from e2ml.evaluation._error_estimation_solution import cross_validation\n",
    "from e2ml.preprocessing import StandardScaler\n",
    "\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the multiclass cross entropy for the given model for every input\n",
    "    (Note: This is the function I should have used)\n",
    "\n",
    "    Parameters:\n",
    "        y_true: np.array true label either as index or onehot encoded\n",
    "        y_pred: np.array containing the predicted labels of the data as probabilities\n",
    "    \"\"\"\n",
    "    #if y contains the information of which class is true as index\n",
    "    if(len(y_true.shape) == 1):\n",
    "        return np.array([np.log(y_pred[i]) for i in y_true])\n",
    "    #if onehot encoded\n",
    "    elif(len(y_true.shape) == 2):\n",
    "        return np.array([-(y_true[i] * np.log(y_pred[i])).sum() for i in range(len(y_pred))])\n",
    "\n",
    "def loadFullData() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get full data available\n",
    "    \"\"\"\n",
    "    initial_molluscs_data = pd.read_csv('../data/initial_molluscs_data.csv')\n",
    "    full_data = initial_molluscs_data\n",
    "    first_batch = pd.read_csv(\"../data/batch1_labels.csv\")\n",
    "    full_data = pd.concat((full_data, first_batch))\n",
    "    second_batch = pd.read_csv(\"../data/batch2_labels.csv\")\n",
    "    full_data = pd.concat((full_data, second_batch))\n",
    "    third_batch = pd.read_csv(\"../data/batch3_labels.csv\")\n",
    "    full_data = pd.concat((full_data, third_batch))\n",
    "    third_batch = pd.read_csv(\"../data/batch3_labels.csv\")\n",
    "    full_data = pd.concat((full_data, third_batch))\n",
    "    third_batch = pd.read_csv(\"../data/batch4_labels.csv\")\n",
    "    full_data = pd.concat((full_data, third_batch))\n",
    "    return full_data\n",
    "\n",
    "def getOneHotEncoding(data:np.array, values:np.array=None):\n",
    "    \"\"\"\n",
    "    One hot encode data\n",
    "\n",
    "    Parameters:\n",
    "        data: np.array containing the (probably categorical) data to be one-hot encoded\n",
    "        values: np.array containing the possible values of the data. Important to ensure the data is encoded the same everytime and can be decoded acordingly\n",
    "    \"\"\"\n",
    "    if(values == None):\n",
    "        values = np.sort(np.unique(data))\n",
    "    enc = np.zeros((len(data), len(values)))\n",
    "    for i, x in enumerate(data):\n",
    "        enc[i, np.where(values == x)[0][0]] = 1\n",
    "    return enc\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    implements the softmax function. Used to normalize the probabiility outputs of the models\n",
    "    \n",
    "    Parameters:\n",
    "        x: array-like that should be normalized\n",
    "    \"\"\"\n",
    "    x = np.array(x)\n",
    "    return np.exp(x) / np.exp(x).sum(axis=1).reshape(x.shape[0],-1)\n",
    "\n",
    "full_data = loadFullData()\n",
    "scaler = StandardScaler().fit(full_data.values[:,1:-1])\n",
    "x = np.concatenate((getOneHotEncoding(full_data[\"Sex\"]), scaler.transform(full_data.values[:,1:-1])), axis=1)\n",
    "y = getOneHotEncoding(full_data[\"Stage of Life\"])\n",
    "\n",
    "def objective_function_MLP(hyperparameters):\n",
    "    num_hiddenlayers = hyperparameters[0]\n",
    "    num_hiddenunits = hyperparameters[1]\n",
    "    alpha = hyperparameters[2]\n",
    "    learning_rate_init = hyperparameters[3]\n",
    "    hidden_layer_sizes = [int(num_hiddenunits) for _ in range(int(num_hiddenlayers))]\n",
    "    model = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, alpha=alpha, learning_rate_init=learning_rate_init, max_iter=1000)\n",
    "    sample_indices = np.arange(len(full_data), dtype=int)\n",
    "    n_folds = 5\n",
    "    train, test = cross_validation(sample_indices=sample_indices, n_folds=n_folds, y=None, random_state=2)\n",
    "    scores = 0\n",
    "    for i in range(n_folds):\n",
    "        with warnings.catch_warnings():#to get rid of the annoying ConverganceWarning\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            model.fit(x[train[i]], y[train[i]])\n",
    "            y_pred = softmax(model.predict_proba(x[test[i]]))\n",
    "            scores += cross_entropy_loss(y[test[i]], y_pred).mean()\n",
    "    return -scores/n_folds,\n",
    "\n",
    "def perform_hyperparameter_optimization_MLP():\n",
    "    nums_hiddenlayers = np.array([0,1,2,3,4])\n",
    "    nums_hiddenunits = np.array([10,33,50,100,150,200,250,300,350,400])\n",
    "    alpha = np.linspace(0.001, 0.00001, 20)\n",
    "    learning_rates = np.linspace(0.01, 0.0001, 20)\n",
    "    candidates = np.array(np.meshgrid(nums_hiddenlayers, nums_hiddenunits, alpha, learning_rates)).T.reshape(-1,4)\n",
    "    metrics_dict = {'gamma': None, 'metric': 'rbf'}\n",
    "    gpr = GaussianProcessRegression(beta=1.e-3, metrics_dict=metrics_dict)\n",
    "    best_params, best_scores = perform_bayesian_optimization(candidates, gpr, acquisition_func=\"ei\", obj_func=objective_function_MLP, n_evals=30, n_random_init=2)\n",
    "    print(best_params, best_scores)\n",
    "    return best_params, best_scores\n",
    "\n",
    "best_params, best_scores = perform_hyperparameter_optimization_MLP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "config: num_hiddenlayers, num_hiddenuntis, learning_rate, alpha\n",
    "[[1.00000000e+00 1.00000000e+02 7.91578947e-03 7.91578947e-04]\n",
    " [2.00000000e+00 1.00000000e+02 5.31052632e-03 1.14210526e-04]\n",
    " [0.00000000e+00 1.00000000e+01 1.00000000e-02 1.00000000e-03]\n",
    " [0.00000000e+00 3.30000000e+01 1.00000000e-02 1.00000000e-03]\n",
    " [0.00000000e+00 5.00000000e+01 1.00000000e-02 1.00000000e-03]\n",
    " [0.00000000e+00 1.50000000e+02 1.00000000e-02 1.00000000e-03]\n",
    " [0.00000000e+00 2.00000000e+02 1.00000000e-02 1.00000000e-03]\n",
    " [0.00000000e+00 2.50000000e+02 1.00000000e-02 1.00000000e-03]\n",
    " [0.00000000e+00 3.00000000e+02 1.00000000e-02 1.00000000e-03]\n",
    " [0.00000000e+00 3.50000000e+02 1.00000000e-02 1.00000000e-03]\n",
    " [0.00000000e+00 4.00000000e+02 1.00000000e-02 1.00000000e-03]\n",
    " [4.00000000e+00 3.50000000e+02 1.00000000e-04 1.00000000e-05]\n",
    " [4.00000000e+00 1.50000000e+02 1.00000000e-04 1.00000000e-05]\n",
    " [4.00000000e+00 1.00000000e+01 1.00000000e-04 1.00000000e-05]\n",
    " [4.00000000e+00 4.00000000e+02 1.00000000e-04 1.00000000e-05]\n",
    " [4.00000000e+00 3.30000000e+01 1.00000000e-04 1.00000000e-05]\n",
    " [4.00000000e+00 2.00000000e+02 1.00000000e-04 1.00000000e-05]\n",
    " [4.00000000e+00 5.00000000e+01 1.00000000e-04 1.00000000e-05]\n",
    " [4.00000000e+00 3.00000000e+02 1.00000000e-04 1.00000000e-05]\n",
    " [4.00000000e+00 2.50000000e+02 1.00000000e-04 1.00000000e-05]\n",
    " [4.00000000e+00 1.00000000e+02 1.00000000e-04 1.00000000e-03]\n",
    " [2.00000000e+00 4.00000000e+02 1.00000000e-04 1.00000000e-05]\n",
    " [2.00000000e+00 3.50000000e+02 1.00000000e-04 1.00000000e-05]\n",
    " [2.00000000e+00 3.00000000e+02 1.00000000e-04 1.00000000e-05]\n",
    " [2.00000000e+00 2.50000000e+02 1.00000000e-04 1.00000000e-05]\n",
    " [2.00000000e+00 2.00000000e+02 1.00000000e-04 1.00000000e-05]\n",
    " [2.00000000e+00 1.50000000e+02 1.00000000e-04 1.00000000e-05]\n",
    " [2.00000000e+00 5.00000000e+01 1.00000000e-04 1.00000000e-05]\n",
    " [2.00000000e+00 3.30000000e+01 1.00000000e-02 1.00000000e-03]\n",
    " [2.00000000e+00 1.00000000e+01 1.00000000e-02 1.00000000e-03]] \n",
    " \n",
    " scores: higher = better\n",
    " [-0.59396657 -0.59218798 -0.70476239 -0.70483637 -0.70496765 -0.7046212\n",
    " -0.70485052 -0.7054535  -0.70507148 -0.70429256 -0.70483418 -0.58633322\n",
    " -0.60650235 -0.95359764 -0.58568454 -0.73858945 -0.59787859 -0.66547465\n",
    " -0.59069548 -0.59327332 -0.58153428 -0.62986713 -0.63434779 -0.64003818\n",
    " -0.64626333 -0.65748636 -0.67012698 -0.75528564 -0.58042515 -0.59160392]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e2ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
